{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXWSRFc2bZ3n"
      },
      "source": [
        "# Baseline Text Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCF8_dkud0mJ"
      },
      "source": [
        "ACEInhibitors dataset from: https://www.dropbox.com/sh/ud5sf1fy6m7o219/AAD9pkY5gYe_XYV2oHDw68uva?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgjZmhZFSDgy"
      },
      "source": [
        "## Libraries and methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ozGvh60UEoU2",
        "outputId": "a4a524db-9272-44b0-fd20-f16b2bd7eee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 793 kB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 381 kB 52.5 MB/s \n",
            "\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install -Uqq ipdb\n",
        "import ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlxUBhCY3Dxm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "path = \"/content/gdrive/MyDrive/Magistrale/Stage/data\"\n",
        "root_dir = \"/content/gdrive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WJuokG0PcAI"
      },
      "outputs": [],
      "source": [
        "#libraries\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, fbeta_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from pathlib import Path  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxP6P9U5VFgD"
      },
      "outputs": [],
      "source": [
        "#preprocessing methods \n",
        "\n",
        "def listToString(s): \n",
        "    str1 = \"\" \n",
        "    for ele in s: \n",
        "        str1 = str1 + \" \" + ele\n",
        "    return str1\n",
        "\n",
        "def clean_text(df, col):\n",
        "    \"\"\"A function for keeping only alpha-numeric characters and replacing all \n",
        "    white space with a single space.\n",
        "    \"\"\"\n",
        "\n",
        "    #[^A-Za-z0-9]+: regex to match a string of characters that are not a letters or numbers\n",
        "    return df[col].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', str(x).lower()))\\\n",
        "                  .apply(lambda x: re.sub('\\s+', ' ', x).strip())\n",
        "\n",
        "def flatten_words(list1d, get_unique=False):\n",
        "    qa = [s.split() for s in list1d]\n",
        "    if get_unique:\n",
        "        return sorted(list(set([w for sent in qa for w in sent])))\n",
        "    else:\n",
        "        return [w for sent in qa for w in sent]\n",
        "\n",
        "def count_words(df, column):\n",
        "    df = df.copy()\n",
        "\n",
        "    # df['n_questionmarks'] = count_pattern(df, text, '\\?')\n",
        "    # df['n_periods'] = count_pattern(df, text, '\\.')\n",
        "    # df['n_apostrophes'] = count_pattern(df, text, '\\'')\n",
        "    # df['first_word'] = df[text_clean].apply(lambda x: split_on_word(x)[0])\n",
        "    # question_words = ['what', 'how', 'why', 'is']\n",
        "    # for w in question_words:\n",
        "    #     col_wc = 'n_' + w\n",
        "    #     col_fw = 'fw_' + w\n",
        "    #     df[col_wc] = count_pattern(df, text_clean, w)\n",
        "    #     df[col_fw] = (df.first_word == w) * 1\n",
        "        \n",
        "    # del df['first_word']\n",
        "    \n",
        "    col_name = \"n_words_in_\" + column\n",
        "    df[col_name] = df[column].apply(lambda x: len(split_on_word(x)))\n",
        "\n",
        "    return df\n",
        "\n",
        "def count_pattern(df, col, pattern):\n",
        "    \"\"\"Count the occurrences of `pattern`\n",
        "    in df[col].\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    return df[col].str.count(pattern)\n",
        "\n",
        "def split_on_word(text):\n",
        "    \"\"\"Use regular expression tokenizer.\n",
        "    Keep apostrophes.\n",
        "    Returns a list of lists, one list for each sentence:\n",
        "        [[word, word], [word, word, ..., word], ...].\n",
        "    \"\"\"\n",
        "    if type(text) is list:\n",
        "        return [regexp_tokenize(sentence, pattern=\"\\w+(?:[-']\\w+)*\") for sentence in text]\n",
        "    else:\n",
        "        return regexp_tokenize(text, pattern=\"\\w+(?:[-']\\w+)*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um7ViP11G0gb"
      },
      "outputs": [],
      "source": [
        "def from_list_values_to_columns(col, k, print=False): \n",
        "\n",
        "#obtaining the unique values \n",
        "\n",
        "  dataset[k][col] = dataset[k][col].apply(eval)\n",
        "\n",
        "  col_dict = {} \n",
        "  for i in dataset[k][col]: #obtain value_count in a dictionary\n",
        "    for j in i:\n",
        "        if j not in col_dict:\n",
        "            col_dict[j] = 1 #new column\n",
        "        else:\n",
        "            col_dict[j] += 1 #update column count\n",
        "\n",
        "  series = pd.Series([x for _list in dataset[k][col] for x in _list]) #reducing its dimensions from 2 to 1 \n",
        "\n",
        "  if print == True:\n",
        "    print(series.value_counts()) #display value count\n",
        "\n",
        "#creating new binary columns \n",
        "\n",
        "  bool_dict = {} #create boolean dict (the binary value for every colum in col_dict and for every row in the dataset[k])\n",
        "  for i, item in enumerate(col_dict.keys()): \n",
        "    bool_dict[item] = dataset[k][col].apply(lambda x: item in x)\n",
        "\n",
        "  return pd.DataFrame(bool_dict).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsKgnQusjJFL"
      },
      "outputs": [],
      "source": [
        "def update_col_names(feature, df, sub_features): \n",
        "\n",
        "  #useful trasnformation for assigning a list to a dataframe cell\n",
        "  l = col_names.index[col_names[\"feature\"] == feature].tolist()\n",
        "  col_names.at[l[0], df] = sub_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pj2i2p6p7sL"
      },
      "outputs": [],
      "source": [
        "def find_documents_about_topic(df, column, new_column, l): \n",
        "\n",
        "  x = df[column][df[column].str.contains('|'.join(l))] #rows in df[column] that contains at least 1 item of \"words\"\n",
        "  \n",
        "  df[new_column] = 0\n",
        "  df.loc[df.index.isin(x.index), new_column] = 1 #assigning 1 to the corresponding rows of x in df\n",
        "\n",
        "  print(\"Number of documents that\", new_column, \":\", len(x))\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl1Ki8mIr_Cm"
      },
      "outputs": [],
      "source": [
        "def evalmetrics(y_test, test_predicted, labels, new_row, res): \n",
        "\n",
        "  #accuracy\n",
        "  accuracy = accuracy_score(y_test, test_predicted)\n",
        "\n",
        "  #precision\n",
        "  precision = precision_score(y_test, test_predicted, average='binary', zero_division=0)\n",
        "  print(\"\\nPrecision with dataset\", new_row[\"df\"], \":\", precision, \"\\n\")\n",
        "\n",
        "  #recall\n",
        "  recall = recall_score(y_test, test_predicted, average='binary', zero_division=0)\n",
        "  print(\"Recall with dataset\", new_row[\"df\"], \":\", recall, \"\\n\")\n",
        "\n",
        "  #f-2 score\n",
        "  f2 = fbeta_score(y_test, test_predicted, average='binary', beta=2)\n",
        "  print(\"F-2 score with dataset\", new_row[\"df\"], \":\", f2, \"\\n\")\n",
        "\n",
        "  #f-3 score\n",
        "  f3 = fbeta_score(y_test, test_predicted, average='binary', beta=3)\n",
        "  print(\"F-3 score with dataset\", new_row[\"df\"], \":\", f3, \"\\n\")\n",
        "\n",
        "  #confusion matrix\n",
        "  cm = confusion_matrix(y_test, test_predicted, labels=labels)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "  disp.plot()\n",
        "  title = new_row[\"model\"] + \" confusion matrix on dataset \" + new_row[\"df\"]\n",
        "  disp.ax_.set_title(title)\n",
        "\n",
        "  #specificity or true negative rate\n",
        "  FP = cm.sum(axis=0) - np.diag(cm)  \n",
        "  FN = cm.sum(axis=1) - np.diag(cm)\n",
        "  TP = np.diag(cm)\n",
        "  TN = cm.sum() - (FP + FN + TP)\n",
        "  TNR = TN/(TN+FP)\n",
        "  print(\"True negative rate with dataset\", new_row[\"df\"], \":\", TNR[1], \"\\n\")\n",
        "\n",
        "  #evaluation metrics\n",
        "  report = classification_report(y_test, test_predicted, output_dict=True)\n",
        "  display(pd.DataFrame(report).transpose().round(decimals=3))\n",
        "\n",
        "  new_row.update({'accuracy' : accuracy, 'precision' : precision, 'recall' : recall, 'true_negative_rate' : TNR[1], 'f2_score' : f2, 'f3_score' : f3})\n",
        "  res = res.append(new_row, ignore_index=True)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JrJYxnhdNwv"
      },
      "outputs": [],
      "source": [
        "def compute_avg_std(res): \n",
        "  \n",
        "  avg_res = pd.DataFrame() \n",
        "\n",
        "  for k in res[\"df\"].unique(): \n",
        "    for j in res[\"model\"].unique(): \n",
        "      condition = (res[\"df\"] == k) & (res[\"model\"] == j)\n",
        "\n",
        "      new_row = {\"df\": k, \"model\":j}\n",
        "      avg_res = avg_res.append(new_row, ignore_index=True)\n",
        "      new_condition = (avg_res[\"df\"] == k) & (avg_res[\"model\"] == j)\n",
        "\n",
        "      for column in res.columns[5:]: #every column from precision column\n",
        "\n",
        "        avg_column = column + \"_avg\"\n",
        "        std_column = column + \"_std\"\n",
        "        \n",
        "        avg_res.loc[new_condition, avg_column] = round(res.loc[condition, column].mean(), 2)\n",
        "        avg_res.loc[new_condition, std_column] = round(res.loc[condition, column].std(), 2)\n",
        "\n",
        "  return avg_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEIHloh6SIXX"
      },
      "source": [
        "## Import of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSqe0rpf1itk"
      },
      "outputs": [],
      "source": [
        "dataset = dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWYQwGtsy9l4"
      },
      "source": [
        "### ACE Inhibitors dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATrKPt4aKj17"
      },
      "outputs": [],
      "source": [
        "#import of data\n",
        "from_drive = True\n",
        "\n",
        "if from_drive == True: \n",
        "  dataset[\"ace\"] = pd.read_csv(path + \"/ACEInhibitors.tsv\", sep='\\t')\n",
        "else: \n",
        "  dataset[\"ace\"] = pd.read_csv('/content/ACEInhibitors.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1JcUfRiO_RP"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"ace\"].shape)\n",
        "pd.set_option('display.max_columns', None)\n",
        "display(dataset[\"ace\"].head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXN22kT2cSWW"
      },
      "source": [
        "#### Columns distribution and selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdFKAC6hXIKp"
      },
      "outputs": [],
      "source": [
        "def print_value_counts(l, df): \n",
        "  for item in l: \n",
        "    print(dataset[df][item].value_counts(), \"\\n\")\n",
        "\n",
        "def rename_columns(names, df): \n",
        "  for key in names: \n",
        "    dataset[df].rename(columns={key : names[key]}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeShaMQ539Qx"
      },
      "outputs": [],
      "source": [
        "#columns \n",
        "print(dataset[\"ace\"].columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6FWnjA_XXZ3"
      },
      "outputs": [],
      "source": [
        "#value counts\n",
        "variables = [\"MH\", \"STAT\", \"VI\", \"IP\", \"DP\", \"FAU\", \"AU\", \"AD\", \"LA\", \"PT\", \"PL\", \"TA\", \"JT\"] \n",
        "print_value_counts(l=variables, df=\"ace\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muFDkIS5TqxF"
      },
      "outputs": [],
      "source": [
        "#target distribution\n",
        "dataset[\"ace\"][\"Label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKXwOMvTYncJ"
      },
      "outputs": [],
      "source": [
        "#renaming columns\n",
        "names = {\"DP\" : \"publication_date\", \"FAU\" : \"full_authors\", \"PT\" : \"publication_type\", \"PL\" : \"publication_place\", \"TA\" : \"journal_title_abbreviation\", \"MH\" : \"mesh_terms\"}\n",
        "rename_columns(names=names, df=\"ace\")\n",
        "\n",
        "#column selection\n",
        "dataset[\"ace\"] = dataset[\"ace\"][[\"publication_date\", \"full_authors\", \"publication_type\", \"publication_place\", \"journal_title_abbreviation\", \"Title\", \"Abstract\", \"mesh_terms\", \"Label\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX9ck0bLzE80"
      },
      "source": [
        "### Chronic obstructive pulmonary disease (COPD) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6sNrFR_zRiK"
      },
      "outputs": [],
      "source": [
        "#import of copd dataset\n",
        "from_drive = True\n",
        "\n",
        "if from_drive == True: \n",
        "  dataset[\"copd\"] = pd.read_csv(path + \"/copd.tsv\", sep='\\t')\n",
        "else: \n",
        "  dataset[\"copd\"] = pd.read_csv('/content/copd.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQWTXUBCznz6"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"copd\"].shape)\n",
        "display(dataset[\"copd\"].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQhCOw5IdeTs"
      },
      "outputs": [],
      "source": [
        "#check if Title is identical to Abstract\n",
        "print(all(dataset[\"copd\"][\"Title\"] == dataset[\"copd\"][\"Abstract\"]))\n",
        "dataset[\"copd\"].drop('Title', inplace=True, axis=1) #drop Title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNl0zaUw0xpy"
      },
      "outputs": [],
      "source": [
        "dataset[\"copd\"][\"Label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3XciWFBz4T0"
      },
      "source": [
        "### Proton Pump Inhibitors dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K8scU9w0IN0"
      },
      "outputs": [],
      "source": [
        "#import of ppi dataset\n",
        "from_drive = True\n",
        "\n",
        "if from_drive == True: \n",
        "  dataset[\"ppi\"] = pd.read_csv(path + \"/ProtonPumpInhibitors.tsv\", sep='\\t')\n",
        "else: \n",
        "  dataset[\"ppi\"] = pd.read_csv('/content/ProtonPumpInhibitors.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlgw-ADX0SR0"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"ppi\"].shape)\n",
        "display(dataset[\"ppi\"].head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImK_TyZX35ji"
      },
      "source": [
        "#### Columns distribution and selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI6ZRkxO4ZVQ"
      },
      "outputs": [],
      "source": [
        "#columns\n",
        "print(dataset[\"ppi\"].columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkSzM1jiZq-F"
      },
      "outputs": [],
      "source": [
        "#value counts\n",
        "print_value_counts(l=variables, df=\"ppi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aPWD7NqZq-F"
      },
      "outputs": [],
      "source": [
        "#target distribution\n",
        "dataset[\"ppi\"][\"Label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNxn67qXBoYl"
      },
      "outputs": [],
      "source": [
        "names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVUSgrxPZq-G"
      },
      "outputs": [],
      "source": [
        "#renaming columns\n",
        "rename_columns(names=names, df=\"ppi\")\n",
        "\n",
        "#column selection\n",
        "dataset[\"ppi\"] = dataset[\"ppi\"][[\"publication_date\", \"full_authors\", \"publication_type\", \"publication_place\", \"journal_title_abbreviation\", \"Title\", \"Abstract\", \"mesh_terms\", \"Label\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXrhD5dBPaG"
      },
      "outputs": [],
      "source": [
        "dataset[\"ppi\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s_fkySjTQg0"
      },
      "source": [
        "## Columns preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTZEx7-6EJuV"
      },
      "source": [
        "### Text preprocessig\n",
        "\n",
        "- Checking missing values, \n",
        "- Concatenating `Title`, `Abstract` and `mesh_terms` in `text`,\n",
        "- Preprocessing the text in `text_clean`,\n",
        "- Removing missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ6KhgGRP9Dy"
      },
      "outputs": [],
      "source": [
        "## Variable mesh_terms\n",
        "\n",
        "for i in dataset: \n",
        "\n",
        "  if i != \"copd\": \n",
        "\n",
        "    #replace missing values with empty list\n",
        "    dataset[i][\"mesh_terms\"] = dataset[i][\"mesh_terms\"].replace(np.NaN, \"[]\")\n",
        "\n",
        "    #from list of strings to string\n",
        "    for index, value in enumerate(dataset[i][\"mesh_terms\"]): \n",
        "      dataset[i].loc[index, \"mesh_terms\"] = listToString(eval(value))\n",
        "      \n",
        "    dataset[i]['mesh_terms'] = dataset[i]['mesh_terms'].str.replace('[/*]',' ')\n",
        "    dataset[i] = count_words(dataset[i], \"mesh_terms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygn7gUZTGrU3"
      },
      "outputs": [],
      "source": [
        "## Variables Title ad Abstract\n",
        "\n",
        "for i in dataset: \n",
        "\n",
        "  #count number of words in Title\n",
        "  if i != \"copd\":\n",
        "    dataset[i] = count_words(dataset[i], \"Title\")\n",
        "\n",
        "  #count number of words in Abstract\n",
        "  dataset[i][\"Abstract\"] = dataset[i][\"Abstract\"].replace(np.NaN, \"[]\") #replace missing values with empty list\n",
        "  dataset[i][\"Abstract\"] = dataset[i][\"Abstract\"].astype(str) \n",
        "  dataset[i] = count_words(dataset[i], \"Abstract\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61ajUVP51fqV"
      },
      "outputs": [],
      "source": [
        "for i in dataset: \n",
        "\n",
        "  #checking missing values\n",
        "  print(\"Dataset\", i, \"shape:\", dataset[i].shape)\n",
        "  print(dataset[i].isnull().sum(axis=0), \"\\n\")\n",
        "\n",
        "  #concatenating Title, Abstract (and mesh_terms)\n",
        "  if i != \"copd\": \n",
        "    dataset[i]['text'] = dataset[i]['Title'] + (dataset[i]['Abstract']).fillna(' ') + (dataset[i]['mesh_terms']).fillna(' ')\n",
        "    dataset[i].drop(['Title', \"Abstract\", \"mesh_terms\"], inplace=True, axis=1)\n",
        "  else:\n",
        "    dataset[i]['text'] = (dataset[i]['Abstract']).fillna(' ')\n",
        "    dataset[i].drop(\"Abstract\", inplace=True, axis=1)\n",
        "    \n",
        "  ## Variable text_clean\n",
        "\n",
        "  #text preprocessing\n",
        "  dataset[i]['text_clean'] = clean_text(dataset[i], 'text')\n",
        "  dataset[i] = count_words(dataset[i], \"text_clean\")\n",
        "\n",
        "  #removing Nan and checking missing values again\n",
        "  #print(\"Missing values after creating \\\"text\\\":\")\n",
        "  #print(dataset[i].isnull().sum(axis=0))\n",
        "  dataset[i] = dataset[i].dropna()\n",
        "  dataset[i] = dataset[i].reset_index()\n",
        "  print(\"New dataset shape:\", dataset[i].shape, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DMXqjelmMq2"
      },
      "source": [
        "### Features preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtRJv4LME4Aa"
      },
      "outputs": [],
      "source": [
        "#creating col_names (used to store columns' names)\n",
        "col_names = pd.DataFrame(columns = [\"feature\", \"ace\", \"ppi\"])\n",
        "col_names = col_names.append({\"feature\": \"publication_type\"}, ignore_index = True)\n",
        "col_names = col_names.append({\"feature\": \"full_authors\"}, ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxca7_zK7Sbh"
      },
      "outputs": [],
      "source": [
        "for i in dataset: \n",
        "\n",
        "  if i != \"copd\": \n",
        "\n",
        "    ## Variable publication_date\n",
        "\n",
        "    #removing the day and maintaining only year and month\n",
        "    for index, content in enumerate(dataset[i][\"publication_date\"]): \n",
        "      if len(nltk.word_tokenize(content)) > 2:\n",
        "        dataset[i].loc[index, \"publication_date\"] = \" \".join(nltk.word_tokenize(content)[0:2])\n",
        "    #print(dataset[i][\"publication_date\"].value_counts(), \"\\n\")\n",
        "    \n",
        "\n",
        "    ## Variable publication_type\n",
        "\n",
        "    temp = from_list_values_to_columns(\"publication_type\", i)\n",
        "    update_col_names(\"publication_type\", i, list(temp.columns))\n",
        "    dataset[i].drop(\"publication_type\", inplace=True, axis=1)\n",
        "    dataset[i] = dataset[i].join(temp)\n",
        "    \n",
        "\n",
        "    ## Variable full_authors\n",
        "\n",
        "    temp = from_list_values_to_columns(\"full_authors\", i)\n",
        "    update_col_names(\"full_authors\", i, list(temp.columns))\n",
        "    dataset[i].drop(\"full_authors\", inplace=True, axis=1)\n",
        "    dataset[i] = dataset[i].join(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPltw6o5i2d6"
      },
      "outputs": [],
      "source": [
        "col_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxKZJGjmUX_"
      },
      "source": [
        "## Topic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OC6fMUDp8yF"
      },
      "source": [
        "### Ace inhibitors related documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tu7ED6AVN20"
      },
      "outputs": [],
      "source": [
        "list1 = [\"ace\", \"angiotensin converting enzyme\"]\n",
        "list2 = [\"alacepril\", \"captopril\", \"zofenopril\", \"enalapril\", \"ramipril\", \n",
        "          \"quinapril\", \"perindopril\", \"lisinopril\", \"benazepril\", \"imidapril\", \n",
        "          \"trandolapril\", \"cilazapril\", \"fosinopril\", \"moexipril\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHIQ6YCXvKUO"
      },
      "outputs": [],
      "source": [
        "dataset[\"ace\"] = find_documents_about_topic(dataset[\"ace\"], \"text_clean\", \"contains_topic\", list1)\n",
        "dataset[\"ace\"] = find_documents_about_topic(dataset[\"ace\"], \"text_clean\", \"contains_other_topic\", list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPkd552qvU2W"
      },
      "outputs": [],
      "source": [
        "print(\"Checking how many documents don't contain any of the searched words:\\n\")\n",
        "print(dataset[\"ace\"][[\"contains_topic\", \"contains_other_topic\"]].eq(0).all(1).value_counts(), \"\\n\")\n",
        "print(\"True -> they don't contain any of the words\")\n",
        "print(\"False -> they contain at list one word\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yVtiww4qSPS"
      },
      "source": [
        "### COPD related documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "posbXaHdqSPU"
      },
      "outputs": [],
      "source": [
        "list1 = [\"chronic obstructive pulmonary disease\", \"copd\"]\n",
        "list2 = [\"chronic obstructive lung disease\", \"cold\", \"chronic obstructive airway disease\", \"coad\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQRrnkPhqSPV"
      },
      "outputs": [],
      "source": [
        "dataset[\"copd\"] = find_documents_about_topic(dataset[\"copd\"], \"text_clean\", \"contains_topic\", list1)\n",
        "dataset[\"copd\"] = find_documents_about_topic(dataset[\"copd\"], \"text_clean\", \"contains_other_topic\", list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rc8azfKqSPW"
      },
      "outputs": [],
      "source": [
        "print(\"Checking how many documents don't contain any of the searched words:\\n\")\n",
        "print(dataset[\"copd\"][[\"contains_topic\", \"contains_other_topic\"]].eq(0).all(1).value_counts(), \"\\n\")\n",
        "print(\"True -> they don't contain any of the words\")\n",
        "print(\"False -> they contain at list one word\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyS_UzulvG6i"
      },
      "source": [
        "### PPI related documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw-xL1I8vG6k"
      },
      "outputs": [],
      "source": [
        "list1 = [\"proton pump inhibitors\", \"ppi\", \"ppis\"]\n",
        "list2 = [\"omeprazole\", \"lansoprazole\", \"dexlansoprazole\", \"esomeprazole\", \"pantoprazole\", \"rabeprazole\", \"ilaprazole\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RsTegFEvG6k"
      },
      "outputs": [],
      "source": [
        "dataset[\"ppi\"] = find_documents_about_topic(dataset[\"ppi\"], \"text_clean\", \"contains_topic\", list1)\n",
        "dataset[\"ppi\"] = find_documents_about_topic(dataset[\"ppi\"], \"text_clean\", \"contains_other_topic\", list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ2P7IWJvG6l"
      },
      "outputs": [],
      "source": [
        "print(\"Checking how many documents don't contain any of the searched words:\\n\")\n",
        "print(dataset[\"ppi\"][[\"contains_topic\", \"contains_other_topic\"]].eq(0).all(1).value_counts(), \"\\n\")\n",
        "print(\"True -> they don't contain any of the words\")\n",
        "print(\"False -> they contain at list one word\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQy_bl_83Ky4"
      },
      "outputs": [],
      "source": [
        "#updating col_names\n",
        "col_names = col_names.append({\"feature\" : [\"contains_topic\"], \"ace\" : [\"contains_topic\"], \"ppi\" : [\"contains_topic\"], \"copd\" : [\"contains_topic\"]}, ignore_index=True) \n",
        "col_names = col_names.append({\"feature\" : [\"contains_other_topic\"], \"ace\" : [\"contains_other_topic\"], \"ppi\" : [\"contains_other_topic\"], \"copd\" : [\"contains_other_topic\"]}, ignore_index=True) \n",
        "col_names = col_names.append({\"feature\" : [\"n_words_in_mesh_terms\"], \"ace\" : [\"n_words_in_mesh_terms\"], \"ppi\" : [\"n_words_in_mesh_terms\"]}, ignore_index=True) \n",
        "col_names = col_names.append({\"feature\" : [\"n_words_in_Title\"], \"ace\" : [\"n_words_in_Title\"], \"ppi\" : [\"n_words_in_Title\"]}, ignore_index=True) \n",
        "col_names = col_names.append({\"feature\" : [\"n_words_in_Abstract\"], \"ace\" : [\"n_words_in_Abstract\"], \"ppi\" : [\"n_words_in_Abstract\"], \"copd\" : [\"n_words_in_Abstract\"]}, ignore_index=True) \n",
        "col_names = col_names.append({\"feature\" : [\"n_words_in_text_clean\"], \"ace\" : [\"n_words_in_text_clean\"], \"ppi\" : [\"n_words_in_text_clean\"], \"copd\" : [\"n_words_in_text_clean\"]}, ignore_index=True) \n",
        "col_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZuSz8xJ_C4a"
      },
      "source": [
        "### Final data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRROYRIK7z7N"
      },
      "source": [
        "\n",
        "#### One hot encode for categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heoTgPbWBOrW"
      },
      "outputs": [],
      "source": [
        "def update_col_names(feature, df, sub_features): \n",
        "\n",
        "  #useful trasnformation for assigning a list to a dataframe cell\n",
        "  l = col_names.index[col_names[\"feature\"] == feature].tolist()\n",
        "  col_names.at[l[0], df] = sub_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzB2zisPgCYd"
      },
      "outputs": [],
      "source": [
        "#one hot encode for categorical variables\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "cat_features = ['publication_date','publication_place','journal_title_abbreviation'] #categorical features\n",
        "\n",
        "#create rows in col_names\n",
        "col_names = col_names.append({\"feature\": \"publication_date\"}, ignore_index = True)\n",
        "col_names = col_names.append({\"feature\": \"publication_place\"}, ignore_index = True)\n",
        "col_names = col_names.append({\"feature\": \"journal_title_abbreviation\"}, ignore_index = True)\n",
        "\n",
        "for i in dataset:\n",
        "\n",
        "  if i != \"copd\": #doesn't have any categorical variable\n",
        "\n",
        "    enc_df = pd.DataFrame(enc.fit_transform(dataset[i][cat_features]).toarray()) #one hot encode df for categorical features\n",
        "    enc_df.columns = enc.get_feature_names(cat_features) #renaming columns of enc_df\n",
        "    print(\"One hot encode dataset shape:\", enc_df.shape)\n",
        "\n",
        "    dataset[i] = dataset[i].join(enc_df)\n",
        "    dataset[i] = dataset[i].drop(cat_features, axis=1)\n",
        "\n",
        "    #updating col_names\n",
        "    update_col_names(feature=\"publication_date\", df=i, \n",
        "                     sub_features=[x for x in enc_df.columns if x.startswith(\"publication_date\")])\n",
        "    update_col_names(feature=\"publication_place\", df=i, \n",
        "                     sub_features=[x for x in enc_df.columns if x.startswith(\"publication_place\")])\n",
        "    update_col_names(feature=\"journal_title_abbreviation\", df=i, \n",
        "                     sub_features=[x for x in enc_df.columns if x.startswith(\"journal\")])\n",
        "    \n",
        "  dataset[i] = dataset[i].drop(\"index\", axis=1)\n",
        "  print(\"Final\", i, \"shape:\", dataset[i].shape, \"\\n\")\n",
        "\n",
        "del enc_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mzovNp2BiEJ"
      },
      "outputs": [],
      "source": [
        "col_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4CghCyYZJso"
      },
      "outputs": [],
      "source": [
        "#saving the preprocessed datasets and col_names in csv files (to use them in \"Dataset statistics by class\")\n",
        "if from_drive == True: \n",
        "  with open(path + \"/preprocessed_ace.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    dataset[\"ace\"].to_csv(f) \n",
        "  with open(path + \"/preprocessed_copd.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    dataset[\"copd\"].to_csv(f) \n",
        "  with open(path + \"/preprocessed_ppi.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    dataset[\"ppi\"].to_csv(f) \n",
        "  with open(path + \"/columns_names.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    col_names.to_csv(f) \n",
        "else: \n",
        "  filepath = Path('/content/preprocessed_acet.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "  dataset[\"ace\"].to_csv(filepath)\n",
        "  filepath = Path('/content/preprocessed_copd.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "  dataset[\"copd\"].to_csv(filepath)\n",
        "  filepath = Path('/content/preprocessed_ppi.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "  dataset[\"ppi\"].to_csv(filepath)\n",
        "  filepath = Path('/content/columns_names.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "  col_names.to_csv(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOW3LSWpBxxH"
      },
      "source": [
        "### Feature importance and feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTqLPSyHB6W6"
      },
      "outputs": [],
      "source": [
        "# creating the random forest that computes feature importance\n",
        "\n",
        "for i in dataset:\n",
        "\n",
        "  forest = RandomForestClassifier(random_state=0)\n",
        "  forest.fit(dataset[i][dataset[i].columns.difference([\"text\", \"text_clean\", \"Label\"])], dataset[i][\"Label\"])\n",
        "\n",
        "  #computing feature importance\n",
        "  importances = forest.feature_importances_\n",
        "\n",
        "  features = list(set(dataset[i].columns) - set([\"text\", \"text_clean\", \"Label\"])) #all columns except text, text_clean and Label\n",
        "\n",
        "  forest_importances = pd.Series(importances, index=features) \n",
        "  forest_importances = forest_importances.sort_values(ascending=False)\n",
        "\n",
        "  print(\"\\nInitial number of features:\", len(forest_importances))\n",
        "\n",
        "  #non-important features\n",
        "  non_important_features = forest_importances.where(forest_importances <= 0.002)\n",
        "  non_important_features = non_important_features.dropna()\n",
        "\n",
        "  #important features\n",
        "  forest_importances = forest_importances.where(forest_importances > 0.002)\n",
        "  forest_importances = forest_importances.dropna()\n",
        "  print(\"Number of features after removing the ones with equal or less than 0.002 importance:\", len(forest_importances), \"\\n\")\n",
        "\n",
        "  #visualization of the top 20 features sorted by importance\n",
        "  fig, ax = plt.subplots()\n",
        "  forest_importances.head(20).plot.bar(ax=ax)\n",
        "  plt.title(\"Feature importances using MDI\")\n",
        "  ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "  plt.show()\n",
        "\n",
        "  #removing non important features from the dataset\n",
        "  dataset[i].drop(non_important_features.index, axis = 1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ge-5Q71lRmZ"
      },
      "outputs": [],
      "source": [
        "#saving the preprocessed datasets and col_names in csv files (to use them in \"Dataset statistics by class\")\n",
        "if from_drive == True: \n",
        "  with open(path + \"/preprocessed_ace2.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    dataset[\"ace\"].to_csv(f) \n",
        "  with open(path + \"/preprocessed_copd2.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    dataset[\"copd\"].to_csv(f) \n",
        "  with open(path + \"/preprocessed_ppi2.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    dataset[\"ppi\"].to_csv(f) \n",
        "else: \n",
        "  filepath = Path('/content/preprocessed_ace2.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "  dataset[\"ace\"].to_csv(filepath)\n",
        "  filepath = Path('/content/preprocessed_copd2.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "  dataset[\"copd\"].to_csv(filepath)\n",
        "  filepath = Path('/content/preprocessed_ppi2.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "  dataset[\"ppi\"].to_csv(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui9nqENRScbb"
      },
      "source": [
        "## Models training and resuls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAkc75h4GNIu"
      },
      "outputs": [],
      "source": [
        "#creating dataset to save results\n",
        "res = pd.DataFrame() \n",
        "\n",
        "#cross validation k=5\n",
        "skf = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
        "\n",
        "pred = pd.DataFrame()\n",
        "\n",
        "for i in dataset: \n",
        "\n",
        "  print(\"\\nDATASET\", i, \"\\n\")\n",
        "\n",
        "  X = dataset[i].loc[:, dataset[i].columns != 'Label']\n",
        "  y = dataset[i][\"Label\"]\n",
        "\n",
        "  j = 0\n",
        "\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "\n",
        "    j = j + 1\n",
        "    print(\"\\nSplit number\", j, \"\\n\")\n",
        "  \n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    #resetting indexes for concat()\n",
        "    X_train = X_train.reset_index(drop=True) \n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "    y_test = y_test.reset_index(drop=True)\n",
        "    \n",
        "    #tf-idf\n",
        "    all_text = X_train[\"text_clean\"].values.tolist() + X_test[\"text_clean\"].values.tolist()\n",
        "    vocab = flatten_words(all_text, get_unique=True)\n",
        "    tfidf = TfidfVectorizer(stop_words='english', vocabulary=vocab)\n",
        "    training_matrix = tfidf.fit_transform(X_train[\"text_clean\"])\n",
        "    test_matrix = tfidf.fit_transform(X_test[\"text_clean\"])\n",
        "\n",
        "    print(\"Training matrix:\", training_matrix.shape)\n",
        "    print(\"Test matrix:\", test_matrix.shape)\n",
        "\n",
        "    X_train = pd.concat([X_train, pd.DataFrame(training_matrix.todense())], axis=1) #add training_matrix to X_train\n",
        "    X_test = pd.concat([X_test, pd.DataFrame(test_matrix.todense())], axis=1)\n",
        "\n",
        "\n",
        "    #scaling data\n",
        "    scaler = StandardScaler()\n",
        "    features = list(set(X_train.columns) - set([\"text\", \"text_clean\", \"Label\"])) #all columns except text, text_clean and Label\n",
        "    X_train = scaler.fit_transform(X_train[features].values)\n",
        "    y_train = y_train.values\n",
        "    X_test = scaler.transform(X_test[features].values)\n",
        "\n",
        "    #####\n",
        "\n",
        "    ## SVM training\n",
        "\n",
        "    svm = SVC(kernel = \"linear\", class_weight=\"balanced\", random_state=1, probability=True)\n",
        "    svm = svm.fit(X_train, y_train)\n",
        "\n",
        "    ## SVM results\n",
        "\n",
        "    #prediction on the test set\n",
        "    test_predicted = svm.predict(X_test) #predicted classes\n",
        "    test_preds = svm.predict_proba(X_test) #predicted probabilities\n",
        "    test_preds = test_preds[:,1]\n",
        "    new_row = {\"df\" : i, \"model\" : \"SVM\", \"fold\": j, \"pred\" : pd.DataFrame(test_preds), \"target\" : pd.DataFrame(y_test)}\n",
        "    pred = pred.append(new_row, ignore_index=True)\n",
        "\n",
        "    #evaluation metrics\n",
        "    print(\"\\nSVM RESULTS:\\n\")\n",
        "    new_row = {\"df\" : i, \"train_shape\" : X_train.shape, \"test_shape\" : X_test.shape, \n",
        "               \"fold\" : j, \"model\" : \"SVM\"}\n",
        "    res = evalmetrics(y_test, test_predicted, svm.classes_, new_row, res)\n",
        "\n",
        "    ######\n",
        "\n",
        "    ## DT training\n",
        "\n",
        "    dt = tree.DecisionTreeClassifier(class_weight=\"balanced\", random_state=1)\n",
        "    dt = dt.fit(X_train, y_train)\n",
        "\n",
        "    ## DT results\n",
        "\n",
        "    #prediction on the test set\n",
        "    test_predicted = dt.predict(X_test) #predicted classes\n",
        "    test_preds = dt.predict_proba(X_test) #predicted probabilities\n",
        "    test_preds = test_preds[:,1]\n",
        "    new_row = {\"df\" : i, \"model\" : \"DT\", \"fold\": j, \"pred\" : pd.DataFrame(test_preds), \"target\" : pd.DataFrame(y_test)}\n",
        "    pred = pred.append(new_row, ignore_index=True)\n",
        "\n",
        "    #evaluation metrics\n",
        "    print(\"\\nDT RESULTS:\\n\")\n",
        "    new_row = {\"df\" : i, \"train_shape\" : X_train.shape, \"test_shape\" : X_test.shape, \n",
        "               \"fold\" : j, \"model\" : \"DT\"}\n",
        "    res = evalmetrics(y_test, test_predicted, svm.classes_, new_row, res)\n",
        "\n",
        "    ######\n",
        "\n",
        "    # RF training\n",
        "\n",
        "    forest = RandomForestClassifier(class_weight=\"balanced\", random_state=1)\n",
        "    forest = forest.fit(X_train, y_train)\n",
        "    ## RF results\n",
        "\n",
        "    #prediction on the test set\n",
        "    test_predicted = forest.predict(X_test) #predicted classes\n",
        "    test_preds = forest.predict_proba(X_test) #predicted probabilities\n",
        "    test_preds = test_preds[:,1]\n",
        "    new_row = {\"df\" : i, \"model\" : \"RF\", \"fold\": j, \"pred\" : pd.DataFrame(test_preds), \"target\" : pd.DataFrame(y_test)}\n",
        "    pred = pred.append(new_row, ignore_index=True)\n",
        "\n",
        "    #evaluation metrics\n",
        "    print(\"\\nRF RESULTS:\\n\")\n",
        "    new_row = {\"df\" : i, \"train_shape\" : X_train.shape, \"test_shape\" : X_test.shape, \n",
        "               \"fold\" : j, \"model\" : \"RF\"}\n",
        "    res = evalmetrics(y_test, test_predicted, svm.classes_, new_row, res)\n",
        "\n",
        "# computation of mean metrics\n",
        "avg_res = compute_avg_std(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn3-Yf_ztDTV"
      },
      "outputs": [],
      "source": [
        "res.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZH-ioDmEfpo"
      },
      "outputs": [],
      "source": [
        "res.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnWv9XTaYe5g"
      },
      "outputs": [],
      "source": [
        "avg_res.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCdr40-QYaxs"
      },
      "outputs": [],
      "source": [
        "avg_res.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k5GnujxEgzq"
      },
      "outputs": [],
      "source": [
        "#saving results in a csv file\n",
        "if from_drive == True: \n",
        "  with open(path + \"/ml_out.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    res.to_csv(f)  \n",
        "  with open(path + \"/ml_avg_out.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "    avg_res.to_csv(f) \n",
        "else: \n",
        "  filepath = Path('/content/ml_out.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "  res.to_csv(filepath)\n",
        "  filepath = Path('/content/ml_avg_out.csv')  \n",
        "  filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "  avg_res.to_csv(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVubn2q-l1Pp"
      },
      "outputs": [],
      "source": [
        "pred1 = pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKBP-r2uA5HW"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def arrange_predictions_and_targets(pred, j): \n",
        "\n",
        "  #create dataframe that contain predictions and targets\n",
        "  temp = pd.DataFrame()\n",
        "  temp[\"prediction_value\"] = pred[\"pred\"][j]\n",
        "  temp[\"target\"] = pred[\"target\"].values.tolist()[j]\n",
        "\n",
        "  #sort in descending order respect to the predictions\n",
        "  temp.sort_values(by=['prediction_value'], inplace=True, ascending=False, ignore_index=True) \n",
        "\n",
        "  #compute the sigmoid function over the predictions (in order to obtain values in [0, 1])\n",
        "  for index, value in enumerate(temp[\"prediction_value\"]):\n",
        "    temp.loc[index, \"sigmoid_value\"] = sigmoid(value)\n",
        "\n",
        "  #convert predictions from real value to 0 or 1\n",
        "  temp.loc[temp[\"sigmoid_value\"] >= 0.5, \"prediction\"] = 1\n",
        "  temp.loc[temp[\"sigmoid_value\"] < 0.5, \"prediction\"] = 0\n",
        "  temp.drop(['prediction_value', \"sigmoid_value\"], inplace=True, axis=1)\n",
        "\n",
        "  #cast dataframe type to int\n",
        "  temp = temp.astype(int)\n",
        "\n",
        "  return temp\n",
        "\n",
        "def getTpTnFpFn(df): \n",
        "\n",
        "  TP = len(df.loc[df[[\"target\", \"prediction\"]].eq(1).all(1)])\n",
        "  TN = len(df.loc[df[[\"target\", \"prediction\"]].eq(0).all(1)])\n",
        "  FP = len(df.loc[(df[\"target\"] == 0) & (df[\"prediction\"] == 1)]) \n",
        "  FN = df.shape[0] - (TP + TN + FP)\n",
        "\n",
        "  return TP, TN, FP, FN\n",
        "\n",
        "def get_rank_at_k(df, total_relevant_docs, K=95): \n",
        "  \n",
        "  relevant_docs = 0\n",
        "  \n",
        "  for j in range(len(df)): \n",
        "    \n",
        "    if df.loc[j, \"target\"] == 1: \n",
        "      relevant_docs = relevant_docs + 1\n",
        "    \n",
        "    recall_k = relevant_docs / total_relevant_docs\n",
        "    \n",
        "    if recall_k >= K/100: \n",
        "      return j\n",
        "\n",
        "  print(\"Error\")\n",
        "  return 0\n",
        "\n",
        "def update_results(pred_df, df, model, K=95): \n",
        "\n",
        "  #compute TP, TN, FP, FN\n",
        "  TP, TN, FP, FN = getTpTnFpFn(pred_df)\n",
        "  retrieved_docs = TP + FP\n",
        "  relevant_docs = TP + FN\n",
        "  not_relevant_docs = TN + FP\n",
        "\n",
        "  #EVALUATION MEASURES AT 95% RECALL\n",
        "\n",
        "  #selection of the rows where recall@95\n",
        "  last_positive_rank = get_rank_at_k(pred_df, relevant_docs) \n",
        "  pred_df_95 = pred_df[:last_positive_rank+1]\n",
        "  pred_df_5 = pred_df[last_positive_rank+1:]\n",
        "  TP_95, TN_95, FP_95, FN_95 = getTpTnFpFn(pred_df_95)\n",
        "  TP_5, TN_5, FP_5, FN_5 = getTpTnFpFn(pred_df_5)\n",
        "\n",
        "  #TRUE NEGATIVE RATE AT 95% RECALL\n",
        "  if not_relevant_docs==0: \n",
        "    print(\"Number of not relevant docs equals to 0\")\n",
        "    TNR_k = 0\n",
        "  else:\n",
        "    TNR_k = TN_5/not_relevant_docs\n",
        "\n",
        "  #PRECISION@95\n",
        "  if retrieved_docs==0: \n",
        "    print(\"Number of retrieved docs equals to 0\")\n",
        "    precision_k = 0\n",
        "  else:\n",
        "    precision_k = TP_95/retrieved_docs \n",
        "\n",
        "  #WSS@95\n",
        "  N = len(pred_df)\n",
        "  WSS_k = ((N - last_positive_rank)/N) - ((100-K)/100)\n",
        "\n",
        "  #update results dataset\n",
        "  condition = (avg_res[\"df\"] == df) & (avg_res[\"model\"] == model)\n",
        "  avg_res.loc[condition, \"true_negative_rate@95\"] = round(TNR_k*100, 3)\n",
        "  avg_res.loc[condition, \"precision@95\"] = round(precision_k*100, 3)\n",
        "  avg_res.loc[condition, \"wss@95\"] = round(WSS_k*100, 3)\n",
        "\n",
        "  print(\"N:\", N)\n",
        "  print(\"Last positive rank:\", last_positive_rank)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VSydHuF7FmX"
      },
      "outputs": [],
      "source": [
        "for j in range(len(pred)): \n",
        "\n",
        "  data = arrange_predictions_and_targets(pred, j)\n",
        "\n",
        "  update_results(data, df=pred.loc[j, \"df\"], model=pred.loc[j, \"model\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsnAhHkDUq_c"
      },
      "outputs": [],
      "source": [
        "avg_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2syaaWuqKJb"
      },
      "outputs": [],
      "source": [
        "with open(path + \"/ml_out_2.csv\", 'w', encoding = 'utf-8-sig') as f:\n",
        "  avg_res.to_csv(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lVN1vN4xWwA"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcVetTKyxZwd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DX9ck0bLzE80",
        "J3XciWFBz4T0"
      ],
      "name": "Traditional_ML_approach.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
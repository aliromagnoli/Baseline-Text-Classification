{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmeGQdA-6OD_"
      },
      "source": [
        "# Neural Network approach with RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8thIKp2gzqP1",
        "outputId": "0e14bff3-a8e2-46d0-86a4-e3f26fef3560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gis6jeY6V-u"
      },
      "source": [
        "## Libraries and data import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJt0lhH6lwNC",
        "outputId": "22b51d4f-340b-4680-9a82-ea78ddadc70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (1.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0) (4.2.0)\n",
            "Requirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch==1.10.0\n",
        "!pip install -U torchtext==0.11.0\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Field, Dataset, Example\n",
        "\n",
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import recall_score, precision_score, fbeta_score\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KElhBiVmaaa",
        "outputId": "f92d51e3-0e85-4a0b-b7fe-454854e28dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#import of the processed dataset and the columns' names\n",
        "from_drive = True\n",
        "dataset = dict()\n",
        "path = \"/content/gdrive/MyDrive/Magistrale/Stage/data\"\n",
        "\n",
        "if from_drive == True: \n",
        "  drive.mount(\"/content/gdrive\")\n",
        "  dataset[\"ace\"] = pd.read_csv(path + \"/preprocessed_ace2.csv\")\n",
        "  dataset[\"copd\"] = pd.read_csv(path + \"/preprocessed_copd2.csv\")\n",
        "  dataset[\"ppi\"] = pd.read_csv(path + \"/preprocessed_ppi2.csv\")\n",
        "else: \n",
        "  dataset[\"ace\"] = pd.read_csv(path + \"/content/preprocessed_ace2.csv\")\n",
        "  dataset[\"copd\"] = pd.read_csv(path + \"/content/preprocessed_copd2.csv\")\n",
        "  dataset[\"ppi\"] = pd.read_csv(path + \"/content/preprocessed_ppi2.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN5V1D64Sgk-"
      },
      "source": [
        "## Data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kN2jlmwWSgk-"
      },
      "outputs": [],
      "source": [
        "#random seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "#FIELD: how the data should be processed\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm',\n",
        "                  include_lengths = True) #in order to know how long the actual sequences are\n",
        "                  #this will cause `batch.text` to now be a tuple with the first element being the sentence \n",
        "                  #(a numericalized tensor that has been padded) and the second element being the actual lengths of the sentences\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "fields = {'label' : LABEL, \"text\" : TEXT}\n",
        "\n",
        "#HYPERPARAMETERS\n",
        "\n",
        "#dataset configuration\n",
        "i = \"ace\" #which dataset\n",
        "clean_text = True #otherwise, it will be used \"text\"\n",
        "\n",
        "#bucket iterator configuration\n",
        "BATCH_SIZE = 128 \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#neural network configuration\n",
        "EMBEDDING_DIM = 100 #size of the dense word vectors (usually 50-250)\n",
        "#in this case, must be equal to that of the pre-trained GloVe vectors\n",
        "HIDDEN_DIM = 256 #size of the hidden states (usually 100-500)\n",
        "OUTPUT_DIM = 1 #number of classes\n",
        "N_LAYERS = 5\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure the pre-trained vectors can be loaded into the model, the `EMBEDDING_DIM` must be equal to that of the pre-trained GloVe vectors loaded earlier."
      ],
      "metadata": {
        "id": "c54oEqgPlpTB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J-7rOPxqDBa",
        "outputId": "ee218a0f-c31d-45ae-e6ce-cae74b4bb438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "#removal and renaming of columns\n",
        "\n",
        "if clean_text == True: \n",
        "  dataset[i].drop(dataset[i].columns.difference(['Label', \"text_clean\"]), 1, inplace=True)\n",
        "  dataset[i].rename(columns={'text_clean': 'text'}, inplace=True)\n",
        "\n",
        "else: \n",
        "  dataset[i].drop(dataset[i].columns.difference(['Label', \"text\"]), 1, inplace=True)\n",
        "\n",
        "dataset[i].rename(columns={'Label': 'label'}, inplace=True)\n",
        "\n",
        "#final dataset with \"label\" and \"text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "ZYgYEQFCqu3P",
        "outputId": "4487130a-8b97-4d26-af8e-d3efbd2d020e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2496, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text\n",
              "0      0  distinct and combined vascular effects of ace ...\n",
              "1      0  computerized surveillance of adverse drug reac...\n",
              "2      0  glomerular size selective dysfunction in niddm...\n",
              "3      0  total arterial compliance in ambulatory hypert...\n",
              "4      0  racial differences in the outcome of left vent..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d0c433a-722f-4797-a56d-8642bdabaaf4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>distinct and combined vascular effects of ace ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>computerized surveillance of adverse drug reac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>glomerular size selective dysfunction in niddm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>total arterial compliance in ambulatory hypert...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>racial differences in the outcome of left vent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d0c433a-722f-4797-a56d-8642bdabaaf4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d0c433a-722f-4797-a56d-8642bdabaaf4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d0c433a-722f-4797-a56d-8642bdabaaf4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "print(dataset[i].shape)\n",
        "dataset[i].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFBCeSvk9cOb"
      },
      "source": [
        "## Methods definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q55Zvmn6rkeJ"
      },
      "outputs": [],
      "source": [
        "class DataFrameDataset(Dataset):\n",
        "  \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
        "  def __init__(self, examples, fields, filter_pred=None):\n",
        "    \"\"\"\n",
        "    Create a dataset from a pandas dataframe of examples and Fields\n",
        "    Arguments:\n",
        "      examples pd.DataFrame: DataFrame of examples\n",
        "      fields {str: Field}: The Fields to use in this tuple. The\n",
        "        string is a field name, and the Field is the associated field.\n",
        "      filter_pred (callable or None): use only exanples for which\n",
        "        filter_pred(example) is true, or use all examples if None.\n",
        "        Default is None\n",
        "    \"\"\"\n",
        "    self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "    if filter_pred is not None:\n",
        "      self.examples = filter(filter_pred, self.examples)\n",
        "    self.fields = dict(fields)\n",
        "    # Unpack field tuples\n",
        "    for n, f in list(self.fields.items()):\n",
        "      if isinstance(n, tuple):\n",
        "        self.fields.update(zip(n, f))\n",
        "        del self.fields[n]\n",
        "\n",
        "class SeriesExample(Example):\n",
        "  \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
        "\n",
        "  @classmethod\n",
        "  def fromSeries(cls, data, fields):\n",
        "    return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "  @classmethod\n",
        "  def fromdict(cls, data, fields):\n",
        "    ex = cls()\n",
        "      \n",
        "    for key, field in fields.items():\n",
        "      if key not in data:\n",
        "        raise ValueError(\"Specified key {} was not found in \"\n",
        "        \"the input data\".format(key))\n",
        "      if field is not None:\n",
        "        setattr(ex, key, field.preprocess(data[key]))\n",
        "      else:\n",
        "        setattr(ex, key, data[key])\n",
        "    return ex"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Different RNN Architecture**\n",
        "\n",
        "Long Short-Term Memory (LSTM) is a type of RNN architecture. \n",
        "\n",
        "Standard RNNs suffer from the vanishing gradient problem. LSTMs overcome this by having an extra recurrent state called a _cell_, $c$ - which can be thought of as the \"memory\" of the LSTM - and using multiple _gates_ which control the flow of information into and out of the memory. We can simply think of the LSTM as a function of $x_t$, $h_t$ and $c_t$, instead of just $x_t$ and $h_t$.\n",
        "\n",
        "$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n",
        "\n",
        "Thus, the model using an LSTM looks something like (with the embedding layers omitted):\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment2.png?raw=1)\n",
        "\n",
        "The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. The final prediction is still, however, only made using the final hidden state, not the final cell state, i.e. $\\hat{y}=f(h_T)$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Bidirectional RNN**\n",
        "\n",
        "The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$. \n",
        "\n",
        "In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor. \n",
        "\n",
        "We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$   \n",
        "\n",
        "The image below shows a bi-directional RNN, with the forward RNN in orange, the backward RNN in green and the linear layer in silver.  \n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment3.png?raw=1)\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Multi-layer RNN**\n",
        "\n",
        "Multi-layer RNNs (also called *deep RNNs*) are another simple concept. The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another *layer*. The hidden state output by the first (bottom) RNN at time-step $t$ will be the input to the RNN above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n",
        "\n",
        "The image below shows a multi-layer unidirectional RNN, where the layer number is given as a superscript. Also note that each layer needs their own initial hidden state, $h_0^L$.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment4.png?raw=1)\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Regularization**\n",
        "\n",
        "Although we've added improvements to our model, each one adds additional parameters. Without going into overfitting into too much detail, the more parameters you have in in your model, the higher the probability that your model will overfit (memorize the training data, causing a low training error but high validation/testing error, i.e. poor generalization to new, unseen examples). To combat this, we use regularization. More specifically, we use a method of regularization called *dropout*. Dropout works by randomly *dropping out* (setting to 0) neurons in a layer during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter and each neuron with dropout applied is considered indepenently. One theory about why dropout works is that a model with parameters dropped out can be seen as a \"weaker\" (less parameters) model. The predictions from all these \"weaker\" models (one for each forward pass) get averaged together withinin the parameters of the model. Thus, your one model can be thought of as an ensemble of weaker models, none of which are over-parameterized and thus should not overfit."
      ],
      "metadata": {
        "id": "ozIeGwFjFOTY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IHPGjaToSglH"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module): #the RNN class is a sub-class of nn.Module\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx): #define the layers of the module \n",
        "      \n",
        "    super().__init__()\n",
        "    \n",
        "    self.embedding = nn.Embedding(vocab_size, \n",
        "                                  embedding_dim, \n",
        "                                  padding_idx = pad_idx) #index of the pad token is passed as an argument to the embedding layer\n",
        "    \n",
        "    self.rnn = nn.LSTM(embedding_dim, #LSTM layer\n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout) #adds dropout on the connections between hidden states in one layer to hidden states in the next layer\n",
        "    #implementing bidirectionality and adding additional layers are done \n",
        "    #by passing values for the num_layers and bidirectional arguments for the RNN/LSTM\n",
        "    \n",
        "    self.fc = nn.Linear(hidden_dim * 2, output_dim) #linear layer\n",
        "    #as the final hidden state of the LSTM has both a forward and a backward component,\n",
        "    #which will be concatenated together, the size of the input to the nn.Linear layer \n",
        "    #is twice that of the hidden dimension size\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout) #dropout layer\n",
        "    #the argument is the probability of dropping out each neuron\n",
        "    #the dropout layer is used within the forward method after each layer we want to apply dropout to\n",
        "\n",
        "      \n",
        "  def forward(self, text, text_lengths): #called when feeding examples into the model\n",
        "  #text_lengths is needed as we are passing the lengths of the sentences \n",
        "  #to be able to use packed padded sequences\n",
        "\n",
        "    #text = [sent len, batch size] (tensor)\n",
        "    #text is a batch of senteces, each having each word converted into a one-hot vector \n",
        "\n",
        "    #the input batch is passed through the embedding layer to get `embedded`, \n",
        "    #which gives us a dense vector representation of our sentences.\n",
        "    embedded = self.dropout(self.embedding(text)) \n",
        "    \n",
        "    #embedded = [sent len, batch size, emb dim] (tensor)\n",
        "    \n",
        "    #PACK SEQUENCE: before we pass our embeddings to the RNN, we need to pack them\n",
        "    #this will cause our RNN to only process the non-padded elements of a sequence\n",
        "    #lengths need to be on CPU\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
        "    \n",
        "    #the RNN will then return packed_output (a packed sequence) as well as the hidden and cell states (both of which are tensors)\n",
        "    packed_output, (hidden, cell) = self.rnn(packed_embedded) \n",
        "    #so LSTM returns the output and a tuple of the final hidden state and the final cell state\n",
        "    \n",
        "    #UNPACK SEQUENCE: unpack of the output sequence to transform it from a packed sequence to a tensor\n",
        "    #the elements of output from padding tokens will be zero tensors (tensors where every element is zero)\n",
        "    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "    \n",
        "    #output = [sent len, batch size, hid dim * num directions] (tensor)\n",
        "    #output is the concatenation of the hidden state from every time step\n",
        "    #output over padding tokens are zero tensors\n",
        "\n",
        "    #hidden = [num layers * num directions, batch size, hid dim]\n",
        "    #cell = [num layers * num directions, batch size, hid dim]\n",
        "    \n",
        "    #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "    #and apply dropout\n",
        "    \n",
        "    #the layers are ordered: [forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]\n",
        "    #as we want the final layer forward and backward hidden states, \n",
        "    #we get the last two hidden layers, hidden[-2,:,:] and hidden[-1,:,:], \n",
        "    #and concatenate them together before passing them to the linear layer (after applying dropout)\n",
        "    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "            \n",
        "    #hidden = [batch size, hid dim * num directions]\n",
        "    \n",
        "    #the last hidden state, hidden, is fed through the linear layer to produce a prediction\n",
        "    return self.fc(hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86qw7_FL1We-"
      },
      "source": [
        "**RNN class notes**: \n",
        "\n",
        "- The tensor `text` should have another dimension due to the one-hot vectors, however PyTorch conveniently stores a one-hot vector as it's index value, i.e. the tensor representing a sentence is just a tensor of the indexes for each token in that sentence. The act of converting a list of tokens into a list of indexes is commonly called *numericalizing*.\n",
        "- Never use dropout on the input or output layers (`text` or `fc` in this case) because you only ever want to use dropout on intermediate layers.\n",
        "- Without packed padded sequences, `hidden` and `cell` are tensors from the last element in the sequence, which will most probably be a pad token, however when using packed padded sequences they are both from the last non-padded element in the sequence.\n",
        "- The `lengths` argument of `packed_padded_sequence` must be a CPU tensor so we explicitly make it one by using `.to('cpu')`.\n",
        "- Usually, there is only the need to unpack output if you are going to use it later on in the model. Although in this case the unpack output is not used, it has been unpacked anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "npsTUoyLSglI"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  \"\"\" \n",
        "  Function that tells how many trainable parameters the model has \n",
        "  so we can compare the number of parameters across different models.\n",
        "  \"\"\"\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PCYo9ODP75Sj"
      },
      "outputs": [],
      "source": [
        "def binary_metrics(preds, y):\n",
        "  \n",
        "  \"\"\"\n",
        "  Returns metrics per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "  \"\"\"\n",
        "\n",
        "  #feed the prediction through a sigmoid layer, squashing the values in [0, 1], then round them to the nearest integer\n",
        "  rounded_preds = torch.round(torch.sigmoid(preds)) \n",
        "\n",
        "  #calculate how many rounded predictions equal the actual labels and average it across the batch\n",
        "  correct = (rounded_preds == y).float() #convert into float for division \n",
        "  acc = correct.sum() / len(correct)\n",
        "\n",
        "  y_true = y.detach().numpy()\n",
        "  y_pred = rounded_preds.detach().numpy()\n",
        "\n",
        "  recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "  precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
        "  f2 = fbeta_score(y_true, y_pred, average='binary', beta=2, zero_division=0)\n",
        "  f3 = fbeta_score(y_true, y_pred, average='binary', beta=3, zero_division=0)\n",
        "\n",
        "  return acc, recall, precision, f2, f3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d8uBilme03-0"
      },
      "outputs": [],
      "source": [
        "def train_model(model, iterator, optimizer, criterion): #iterates over all examples, one batch at a time\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  epoch_recall = 0\n",
        "  epoch_precision = 0\n",
        "  epoch_f2 = 0\n",
        "  epoch_f3 = 0\n",
        "  \n",
        "  model.train() #put the model in \"training mode\", which turns on dropout and batch normalization\n",
        "  \n",
        "  for batch in iterator: #for each batch\n",
        "    \n",
        "    optimizer.zero_grad() #we first zero the gradients\n",
        "\n",
        "    #separete batch.text before passing it to the model\n",
        "    #batch.text is a tuple with the first element being the numericalized tensor \n",
        "    #and the second element being the actual lengths of each sequence\n",
        "    text, text_lengths = batch.text\n",
        "\n",
        "    #feed the batch of sentences,text, and their lenghts, text_lenghts into the model \n",
        "    predictions = model(text, text_lengths).squeeze(1)\n",
        "    #squeeze is needed as the predictions are initially size [batch size, 1]\n",
        "    #and we need to remove the dimension of size 1, as PyTorch expects the \n",
        "    #predictions input to our criterion function to be of size [batch size]    \n",
        "    \n",
        "    #loss and accuracy are then calculated using the predictions and the labels, batch.label, \n",
        "    #with the loss being averaged over all examples in the batch    \n",
        "    \n",
        "    #computation of loss \n",
        "    loss = criterion(predictions, batch.label)\n",
        "    #criterion expects both input to be FloatTensors\n",
        "    #that's wht in the label field we set dtype=torch.float \n",
        "    \n",
        "    #computation of accuracy\n",
        "    acc, recall, precision, f2, f3 = binary_metrics(predictions.cpu(), batch.label.cpu())\n",
        "\n",
        "    loss.backward() #calculate the gradient of each parameter\n",
        "    \n",
        "    optimizer.step() #update the parameters using the gradients and optimizer algorithm\n",
        "\n",
        "    #the loss and accuracy are accumulated across the epoch, \n",
        "    #the .item() method is used to extract a scalar from a tensor which only contains a single value\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "    epoch_recall += recall.item()\n",
        "    epoch_precision += precision.item()\n",
        "    epoch_f2 += f2.item()\n",
        "    epoch_f3 += f3.item()\n",
        "\n",
        "  #return the loss and accuracy, averaged across the epochs\n",
        "  #the len of an iterator is the number of batches in the iterator\n",
        "    \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_recall / len(iterator), epoch_precision / len(iterator), epoch_f2 / len(iterator), epoch_f3 / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OT9rn6chJoY"
      },
      "source": [
        "All layers have their parameters initialized to random values, unless explicitly specified.\n",
        "\n",
        "- **Embedding layer**: is used to transform the sparse one-hot vector into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). It is simply a single fully connected layer. \n",
        "As well as reducing the dimensionality of the input to the RNN, there is the theory that words with similar meaning are mapped close together in this dense vector space.\n",
        "\n",
        "- **RNN**: takes in the dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
        "\n",
        "- **Linear layer**: takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD8ct0MgJuJS"
      },
      "source": [
        "**Train_model method notes**: \n",
        "\n",
        "- Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
        "\n",
        "- You do not need to do `model.forward(batch.text)`, simply calling the model works.\n",
        "\n",
        "- When initializing the `LABEL` field, we set `dtype=torch.float`. This is because TorchText sets tensors to be `LongTensor`s by default, however our criterion expects both inputs to be `FloatTensor`s. \n",
        "The alternative method of doing this would be to do the conversion inside the `train` function by passing `batch.label.float()` instad of `batch.label` to the criterion. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "waDIWXWay3vH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, iterator, criterion): #similar to train, without the the update of the parameters\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  epoch_recall = 0\n",
        "  epoch_precision = 0\n",
        "  epoch_f2 = 0\n",
        "  epoch_f3 = 0\n",
        "  \n",
        "  model.eval() #puts the model in \"evaluation mode\", which turns off dropout and batch normalization\n",
        "  \n",
        "  with torch.no_grad(): #in order to not calculate gradients\n",
        "\n",
        "    for batch in iterator:\n",
        "\n",
        "      #separate batch.text\n",
        "      text, text_lengths = batch.text\n",
        "\n",
        "      predictions = model(text, text_lengths).squeeze(1)\n",
        "      \n",
        "      loss = criterion(predictions, batch.label)\n",
        "      \n",
        "      acc, recall, precision, f2, f3 = binary_metrics(predictions.cpu(), batch.label.cpu())\n",
        "      \n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "      epoch_recall += recall.item()\n",
        "      epoch_precision += precision.item()\n",
        "      epoch_f2 += f2.item()\n",
        "      epoch_f3 += f3.item()\n",
        "      \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_recall / len(iterator), epoch_precision / len(iterator), epoch_f2 / len(iterator), epoch_f3 / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cktTGJzgSglK"
      },
      "source": [
        "**Evaluate_model method notes:**\n",
        "\n",
        "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
        "\n",
        "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w8g23ObuSglK"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  \"\"\"\n",
        "  Returns how long an epoch takes, in order to compare training times between models\n",
        "  \"\"\"\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "Er5fo9HJEIR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cross validation 10 folds\n",
        "for j in range(10): \n",
        "\n",
        "  ##### TRAIN-TEST-VAL SPLIT #####\n",
        "\n",
        "  SEED = random.randint(1, 3000)\n",
        "\n",
        "  # TRAIN-TEST SPLIT\n",
        "  train, test = train_test_split(dataset[i], train_size=0.5, random_state=random.seed(SEED), shuffle=True, stratify=dataset[i]['label'])\n",
        "\n",
        "  # OVERSAMPLING ON TRAINING SET\n",
        "\n",
        "  #instantiating the random over sampler \n",
        "  ros = RandomOverSampler(sampling_strategy=0.5, random_state=SEED)\n",
        "  #resampling X, y\n",
        "  X_ros, y_ros = ros.fit_resample(train[\"text\"].values.reshape(-1, 1), train[\"label\"].values)\n",
        "  #creation of oversampled training set\n",
        "  oversampled_train = pd.DataFrame(X_ros, columns=[\"text\"])\n",
        "  oversampled_train[\"label\"] = y_ros\n",
        "\n",
        "  # PYTORCH DATASET \n",
        "  train_data = DataFrameDataset(oversampled_train, fields)\n",
        "  df = DataFrameDataset(test, fields)\n",
        "  \n",
        "  # TEST-VAL SPLIT\n",
        "  test_data, valid_data = df.split(split_ratio=0.8, stratified=True, strata_field='label', random_state = random.seed(SEED))\n",
        "\n",
        "  # CREATION OF VOCABULARY\n",
        "\n",
        "  #VOCABULARY: look up table where every unique word in your data set has a corresponding _index_ (an integer).\n",
        "  #each _index_ is used to construct a _one-hot_ vector for each word.\n",
        "  TEXT.build_vocab(train_data,\n",
        "                  vectors = \"glove.6B.100d\", #use of pre-trained word embeddings\n",
        "                  unk_init = torch.Tensor.normal_) #initialize words in pre-trained embeddings randomly via gaussian distribution\n",
        "\n",
        "  LABEL.build_vocab(train_data)\n",
        "\n",
        "  # PRINTS\n",
        "  if j == 0: \n",
        "    #new class distribution \n",
        "    print(\"Label distribution in training set:\", Counter(y_ros), \"\\n\")\n",
        "\n",
        "    print(f'Number of training examples: {len(train_data)}')\n",
        "    print(f'Number of testing examples: {len(test_data)}')\n",
        "    print(f'Number of validation examples: {len(valid_data)}\\n')\n",
        "\n",
        "    #checking an example\n",
        "    print(\"Training example:\", vars(train_data.examples[0]), \"\\n\")\n",
        "\n",
        "    print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "    print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\\n\")\n",
        "\n",
        "    print(\"Most common words:\", TEXT.vocab.freqs.most_common(20))\n",
        "\n",
        "    #see the vocabulary directly using either stoi (string to int) or itos (int to string)\n",
        "    print(TEXT.vocab.itos[:10])\n",
        "\n",
        "    #check the labels, ensuring 0 is for negative and 1 is for positive\n",
        "    #LABEL.vocab.stoi = {1:1, 0:0}\n",
        "    print(LABEL.vocab.stoi)\n",
        "\n",
        "  # CREATION OF THE ITERATORS\n",
        "\n",
        "  #ITERATORS: you iterate over iterators in the training/evaluation loop, \n",
        "  #and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "\n",
        "  #BUCKET ITERATORS: special type of iterator that will return a batch of examples \n",
        "  #where each example is of a similar length, minimizing the amount of padding per example.\n",
        "\n",
        "  train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device, \n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True) #False, ##for packed padded sequences all of the tensors within a batch need to be sorted by their lengths\n",
        "\n",
        "  \n",
        "  ##### BUILD THE MODEL #####\n",
        "\n",
        "  # CREATION OF RNN CLASS' INSTANCE\n",
        "\n",
        "  INPUT_DIM = len(TEXT.vocab) #input dimension: dimension of the one-hot vectors (equal to the vocabulary size)\n",
        "  PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] #get the pad token index from the vocabulary\n",
        "  #this allows to get the actual string representing the pad token from the field's pad_token attribute, which is <pad> by default\n",
        "\n",
        "  model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, \n",
        "              BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "  \n",
        "  if j == 0: \n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters\\n')\n",
        "\n",
        "  # USE OF WORD EMBEDDING IN THE MODEL \n",
        "\n",
        "  #retrieve the embeddings from the field's vocab\n",
        "  pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "  #check the embeddings are the correct size, [vocab size, embedding dim]\n",
        "  if j == 0:\n",
        "    print(\"Pretrained embeddings shape:\", pretrained_embeddings.shape, \"\\n\")\n",
        "\n",
        "  #replace the initial weights of the embedding layer with the pre-trained embeddings\n",
        "  model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "  # INITIALIZATION OF <UNK> AND <PAD> TO ALL ZEROS \n",
        "\n",
        "  #get the index of the <unk> tokens\n",
        "  UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "  #initialize <unk> and <pad> tokens to all zeros\n",
        "  model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "  model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "  #check that the first two rows of the embedding weights matrix have been set to zeros\n",
        "  #print(model.embedding.weight.data)\n",
        "\n",
        "  ##### TRAIN THE MODEL #####\n",
        "\n",
        "  # TRAINING PARAMETERS\n",
        "\n",
        "  #OPTIMIZER: algorithm we use to update the parameters of the module. \n",
        "  #chosen optimizer: Adam\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  #CRITERION: loss function\n",
        "  #chosen loss function: binary cross entropy with logits\n",
        "  weights = [1 - (len(dataset[i][dataset[i][\"label\"]==0]) / dataset[i].shape[0])]\n",
        "  weight = torch.FloatTensor(weights)\n",
        "  criterion = nn.BCEWithLogitsLoss(weight=weight)\n",
        "  #criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  #place the model and the criterion on the GPU (if we have one)\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  #train the model through multiple epochs\n",
        "  #EPOCH: complete pass through all examples in the training and validation sets\n",
        "\n",
        "  N_EPOCHS = 5\n",
        "\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  # TRAINING THE MODEL\n",
        "\n",
        "  print(\"\\nFOLD\", j+1, \"\\n\")\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    #training \n",
        "    train_loss, train_acc, train_recall, train_precision, train_f2, train_f3= train_model(model, train_iterator, optimizer, criterion)\n",
        "    #evaluation\n",
        "    valid_loss, valid_acc, valid_recall, valid_precision, valid_f2, valid_f3 = evaluate_model(model, valid_iterator, criterion)\n",
        "    \n",
        "    #computation of time\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    #at each epoch, if the validation loss is the best seen so far,\n",
        "    #we'll save the parameters of the model and then, after training has finished,\n",
        "    #we'll use that model on the test set\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    #results for each epoch\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train Recall: {train_recall*100:.2f}% | Train Precision: {train_precision*100:.2f}% | Train F2: {train_f2*100:.2f}% | Train F3: {train_f3*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. Recall: {valid_recall*100:.2f}% | Val. Precision: {valid_precision*100:.2f}% | Val. F2: {valid_f2*100:.2f}% | Val. F3: {valid_f3*100:.2f}%')\n",
        "\n",
        "  # RESULTS ON TEST SET\n",
        "\n",
        "  #test loss and accuracy (using parameters that gave the best validation loss)\n",
        "  model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "  test_loss, test_acc, test_recall, test_precision, test_f2, test_f3 = evaluate_model(model, test_iterator, criterion)\n",
        "  print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Test Recall: {test_recall*100:.2f}% | Test Precision: {test_precision*100:.2f}% | Test F2: {test_f2*100:.2f}% | Test F3: {test_f3*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_STeP9xfpaL",
        "outputId": "1677d5ae-7dcb-4cb0-8d3f-3fdb9e515079"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution in training set: Counter({0: 1229, 1: 614}) \n",
            "\n",
            "Number of training examples: 1843\n",
            "Number of testing examples: 998\n",
            "Number of validation examples: 250\n",
            "\n",
            "Training example: {'label': 0, 'text': ['differences', 'between', 'nisoldipine', 'and', 'lisinopril', 'on', 'glomerular', 'filtration', 'rates', 'and', 'albuminuria', 'in', 'hypertensive', 'iddm', 'patients', 'with', 'diabetic', 'nephropathy', 'during', 'the', 'first', 'year', 'of', 'treatment', 'our', 'objective', 'was', 'to', 'compare', 'the', 'effect', 'of', 'a', 'long', 'acting', 'calcium', 'antagonist', 'nisoldipine', 'versus', 'an', 'ace', 'inhibitor', 'lisinopril', 'on', 'albuminuria', 'arterial', 'blood', 'pressure', 'and', 'glomerular', 'filtration', 'rate', 'gfr', 'in', 'hypertensive', 'iddm', 'patients', 'with', 'diabetic', 'nephropathy', 'we', 'performed', 'a', '1', 'year', 'double', 'blind', 'double', 'dummy', 'randomized', 'controlled', 'study', 'comparing', 'nisoldipine', '20', '40', 'mg', 'once', 'daily', 'with', 'lisinopril', '10', '20', 'mg', 'once', 'daily', 'in', '52', 'hypertensive', 'iddm', 'subjects', 'with', 'diabetic', 'nephropathy', 'three', 'patients', 'dropped', 'out', 'and', 'results', 'for', 'the', 'remaining', '49', '25', 'nisoldipine', '24', 'lisinopril', 'are', 'presented', 'diuretics', 'were', 'required', 'in', '10', 'nisoldipine', 'and', '8', 'lisinopril', 'treated', 'patients', 'every', '3', 'months', '24', 'h', 'ambulatory', 'blood', 'pressure', 'tm2420', 'a', 'd', 'tokyo', 'japan', 'and', 'albuminuria', 'in', 'three', '24', 'h', 'samples', 'enzyme', 'immunoassay', 'were', 'measured', 'gfr', '51cr', 'edta', 'plasma', 'clearance', 'was', 'recorded', 'every', '6', 'months', 'mean', 'arterial', 'blood', 'pressure', '24', 'h', 'was', 'reduced', 'from', 'mean', 'se', '108', '3', 'mmhg', 'at', 'baseline', 'to', '101', '2', 'in', 'average', 'during', 'treatment', 'in', 'the', 'lisinopril', 'group', 'and', 'from', '105', '2', 'to', '103', '2', 'in', 'the', 'nisoldipine', 'group', 'p', '0', '06', 'comparing', 'changes', 'in', 'the', 'two', 'groups', 'albuminuria', 'was', 'reduced', '47', '95', 'ci', '21', '65', 'in', 'the', 'lisinopril', 'group', 'versus', 'an', 'increase', 'of', '11', '3', 'to', '27', 'in', 'the', 'nisoldipine', 'group', 'p', '0', '001', 'fractional', 'albumin', 'clearance', 'was', 'reduced', '37', '95', 'ci', '4', '59', 'in', 'the', 'lisinopril', 'versus', 'an', 'increase', 'of', '35', '8', '69', 'in', 'the', 'nisoldipine', 'group', 'p', '0', '01', 'gfr', 'decreased', 'from', '85', '5', 'ml', 'x', 'min', '1', 'x', '1', '73', 'm', '2', 'to', '73', '5', 'in', 'the', 'lisinopril', 'group', 'and', 'from', '84', '6', 'to', '80', '7', 'in', 'the', 'nisoldipine', 'group', 'p', '0', '05', 'the', 'effect', 'of', 'study', 'medication', 'on', 'albuminuria', 'and', 'gfr', 'was', 'independent', 'of', 'changes', 'in', 'systemic', 'blood', 'pressure', 'and', 'baseline', 'variables', 'in', 'multiple', 'regression', 'analyses', 'in', 'summary', 'lisinopril', 'reduced', 'albuminuria', 'but', 'also', 'gfr', 'to', 'a', 'greater', 'extent', 'than', 'did', 'nisoldipine', 'in', 'hypertensive', 'iddm', 'patients', 'with', 'diabetic', 'nephropathy', 'during', 'the', '1st', 'year', 'of', 'treatment', 'longer', 'follow', 'up', 'is', 'required', 'to', 'clarify', 'whether', 'these', 'drugs', 'have', 'different', 'renoprotective', 'effects', 'adult', 'albuminuria', 'angiotensin', 'converting', 'enzyme', 'inhibitors', 'therapeutic', 'use', 'antihypertensive', 'agents', 'therapeutic', 'use', 'blood', 'pressure', 'drug', 'effects', 'calcium', 'channel', 'blockers', 'therapeutic', 'use', 'chi', 'square', 'distribution', 'diabetes', 'mellitus', 'type', '1', 'physiopathology', 'diabetic', 'angiopathies', 'drug', 'therapy', 'diabetic', 'nephropathies', 'physiopathology', 'double', 'blind', 'method', 'female', 'glomerular', 'filtration', 'rate', 'drug', 'effects', 'humans', 'hypertension', 'drug', 'therapy', 'lisinopril', 'therapeutic', 'use', 'male', 'nisoldipine', 'therapeutic', 'use', 'patient', 'compliance', 'renin', 'blood', 'time', 'factors']} \n",
            "\n",
            "Unique tokens in TEXT vocabulary: 9936\n",
            "Unique tokens in LABEL vocabulary: 2\n",
            "\n",
            "Most common words: [('the', 18884), ('of', 17885), ('and', 15069), ('in', 13766), ('with', 9233), ('patients', 8550), ('to', 8197), ('a', 6425), ('0', 6144), ('was', 5143), ('drug', 4785), ('were', 4765), ('effects', 4567), ('1', 4414), ('use', 4248), ('therapeutic', 3964), ('p', 3591), ('therapy', 3537), ('or', 3490), ('treatment', 3250)]\n",
            "['<unk>', '<pad>', 'the', 'of', 'and', 'in', 'with', 'patients', 'to', 'a']\n",
            "defaultdict(None, {0: 0, 1: 1})\n",
            "The model has 8,035,137 trainable parameters\n",
            "\n",
            "Pretrained embeddings shape: torch.Size([9936, 100]) \n",
            "\n",
            "\n",
            "FOLD 1 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 66.72% | Train Recall: 4.11% | Train Precision: 2.33% | Train F2: 3.57% | Train F3: 3.82%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 68.02% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 67.65% | Train Recall: 10.16% | Train Precision: 13.18% | Train F2: 9.96% | Train F3: 10.04%\n",
            "\t Val. Loss: 0.002 |  Val. Acc: 98.79% | Val. Recall: 33.33% | Val. Precision: 33.33% | Val. F2: 33.33% | Val. F3: 33.33%\n",
            "Epoch: 04 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.008 | Train Acc: 76.93% | Train Recall: 35.49% | Train Precision: 50.39% | Train F2: 35.18% | Train F3: 35.24%\n",
            "\t Val. Loss: 0.011 |  Val. Acc: 28.46% | Val. Recall: 100.00% | Val. Precision: 2.24% | Val. F2: 10.08% | Val. F3: 17.92%\n",
            "Epoch: 05 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.008 | Train Acc: 77.45% | Train Recall: 57.04% | Train Precision: 56.27% | Train F2: 54.52% | Train F3: 55.60%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 89.98% | Val. Recall: 83.33% | Val. Precision: 10.99% | Val. F2: 33.94% | Val. F3: 47.12%\n",
            "Test Loss: 0.002 | Test Acc: 98.04% | Test Recall: 2.50% | Test Precision: 12.50% | Test F2: 2.98% | Test F3: 2.72%\n",
            "\n",
            "FOLD 2 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.010 | Train Acc: 64.79% | Train Recall: 6.06% | Train Precision: 1.68% | Train F2: 3.98% | Train F3: 4.81%\n",
            "\t Val. Loss: 0.008 |  Val. Acc: 98.36% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 67.86% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.004 |  Val. Acc: 98.36% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 69.48% | Train Recall: 33.74% | Train Precision: 38.76% | Train F2: 31.74% | Train F3: 32.57%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 83.15% | Val. Recall: 25.00% | Val. Precision: 4.35% | Val. F2: 12.82% | Val. F3: 16.95%\n",
            "Epoch: 04 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.007 | Train Acc: 82.52% | Train Recall: 43.91% | Train Precision: 57.03% | Train F2: 43.23% | Train F3: 43.31%\n",
            "\t Val. Loss: 0.004 |  Val. Acc: 93.54% | Val. Recall: 25.00% | Val. Precision: 9.09% | Val. F2: 18.52% | Val. F3: 21.28%\n",
            "Epoch: 05 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.006 | Train Acc: 87.16% | Train Recall: 58.53% | Train Precision: 60.69% | Train F2: 56.64% | Train F3: 57.20%\n",
            "\t Val. Loss: 0.003 |  Val. Acc: 93.58% | Val. Recall: 25.00% | Val. Precision: 11.11% | Val. F2: 20.00% | Val. F3: 22.22%\n",
            "Test Loss: 0.004 | Test Acc: 91.86% | Test Recall: 20.42% | Test Precision: 7.49% | Test F2: 14.95% | Test F3: 17.22%\n",
            "\n",
            "FOLD 3 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 67.97% | Train Recall: 0.68% | Train Precision: 3.33% | Train F2: 0.81% | Train F3: 0.74%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 98.36% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 71.72% | Train Recall: 17.44% | Train Precision: 14.42% | Train F2: 16.44% | Train F3: 16.91%\n",
            "\t Val. Loss: 0.003 |  Val. Acc: 92.35% | Val. Recall: 12.50% | Val. Precision: 5.00% | Val. F2: 9.62% | Val. F3: 10.87%\n",
            "Epoch: 03 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.008 | Train Acc: 72.54% | Train Recall: 34.69% | Train Precision: 33.17% | Train F2: 32.19% | Train F3: 33.30%\n",
            "\t Val. Loss: 0.003 |  Val. Acc: 96.37% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 04 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.005 | Train Acc: 83.98% | Train Recall: 65.97% | Train Precision: 62.69% | Train F2: 63.77% | Train F3: 64.81%\n",
            "\t Val. Loss: 0.003 |  Val. Acc: 85.96% | Val. Recall: 50.00% | Val. Precision: 8.70% | Val. F2: 25.64% | Val. F3: 33.90%\n",
            "Epoch: 05 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.005 | Train Acc: 85.06% | Train Recall: 73.24% | Train Precision: 60.69% | Train F2: 69.61% | Train F3: 71.33%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 75.54% | Val. Recall: 50.00% | Val. Precision: 5.41% | Val. F2: 18.87% | Val. F3: 27.40%\n",
            "Test Loss: 0.002 | Test Acc: 97.73% | Test Recall: 15.00% | Test Precision: 8.33% | Test F2: 11.65% | Test F3: 13.02%\n",
            "\n",
            "FOLD 4 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.010 | Train Acc: 65.80% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.009 | Train Acc: 68.02% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 97.99% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.008 | Train Acc: 77.65% | Train Recall: 42.35% | Train Precision: 49.41% | Train F2: 40.74% | Train F3: 41.43%\n",
            "\t Val. Loss: 0.001 |  Val. Acc: 97.19% | Val. Recall: 16.67% | Val. Precision: 16.67% | Val. F2: 16.67% | Val. F3: 16.67%\n",
            "Epoch: 04 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.010 | Train Acc: 70.74% | Train Recall: 46.09% | Train Precision: 45.77% | Train F2: 39.62% | Train F3: 41.22%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 84.38% | Val. Recall: 16.67% | Val. Precision: 2.63% | Val. F2: 8.06% | Val. F3: 10.87%\n",
            "Epoch: 05 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.007 | Train Acc: 80.39% | Train Recall: 31.05% | Train Precision: 41.26% | Train F2: 32.11% | Train F3: 31.54%\n",
            "\t Val. Loss: 0.004 |  Val. Acc: 86.39% | Val. Recall: 83.33% | Val. Precision: 8.33% | Val. F2: 28.03% | Val. F3: 40.74%\n",
            "Test Loss: 0.001 | Test Acc: 97.41% | Test Recall: 18.75% | Test Precision: 14.58% | Test F2: 17.00% | Test F3: 17.77%\n",
            "\n",
            "FOLD 5 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 66.61% | Train Recall: 0.85% | Train Precision: 3.33% | Train F2: 1.00% | Train F3: 0.91%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 67.78% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 97.97% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.008 | Train Acc: 76.95% | Train Recall: 31.34% | Train Precision: 55.09% | Train F2: 33.11% | Train F3: 32.17%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 88.31% | Val. Recall: 16.67% | Val. Precision: 2.78% | Val. F2: 8.33% | Val. F3: 11.11%\n",
            "Epoch: 04 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.007 | Train Acc: 81.30% | Train Recall: 39.38% | Train Precision: 51.72% | Train F2: 39.13% | Train F3: 39.19%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 77.94% | Val. Recall: 50.00% | Val. Precision: 4.55% | Val. F2: 16.67% | Val. F3: 25.00%\n",
            "Epoch: 05 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.005 | Train Acc: 85.39% | Train Recall: 46.06% | Train Precision: 54.19% | Train F2: 45.92% | Train F3: 45.87%\n",
            "\t Val. Loss: 0.002 |  Val. Acc: 95.53% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Test Loss: 0.002 | Test Acc: 93.14% | Test Recall: 22.92% | Test Precision: 4.79% | Test F2: 11.55% | Test F3: 14.77%\n",
            "\n",
            "FOLD 6 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 58.02% | Train Recall: 0.45% | Train Precision: 4.17% | Train F2: 0.55% | Train F3: 0.49%\n",
            "\t Val. Loss: 0.008 |  Val. Acc: 98.40% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 67.97% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 98.40% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 71.56% | Train Recall: 23.29% | Train Precision: 35.65% | Train F2: 23.57% | Train F3: 23.35%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 87.15% | Val. Recall: 25.00% | Val. Precision: 3.57% | Val. F2: 11.36% | Val. F3: 15.62%\n",
            "Epoch: 04 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.007 | Train Acc: 84.47% | Train Recall: 50.59% | Train Precision: 48.85% | Train F2: 48.24% | Train F3: 49.11%\n",
            "\t Val. Loss: 0.003 |  Val. Acc: 97.58% | Val. Recall: 25.00% | Val. Precision: 25.00% | Val. F2: 25.00% | Val. F3: 25.00%\n",
            "Epoch: 05 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.007 | Train Acc: 83.19% | Train Recall: 48.30% | Train Precision: 54.02% | Train F2: 45.75% | Train F3: 46.51%\n",
            "\t Val. Loss: 0.003 |  Val. Acc: 97.60% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Test Loss: 0.002 | Test Acc: 97.92% | Test Recall: 12.92% | Test Precision: 12.50% | Test F2: 12.82% | Test F3: 12.87%\n",
            "\n",
            "FOLD 7 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 63.51% | Train Recall: 6.67% | Train Precision: 1.25% | Train F2: 3.57% | Train F3: 4.65%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 67.88% | Train Recall: 0.80% | Train Precision: 5.71% | Train F2: 0.97% | Train F3: 0.88%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 94.79% | Val. Recall: 16.67% | Val. Precision: 8.33% | Val. F2: 13.89% | Val. F3: 15.15%\n",
            "Epoch: 03 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.006 | Train Acc: 82.83% | Train Recall: 54.11% | Train Precision: 57.40% | Train F2: 51.71% | Train F3: 52.78%\n",
            "\t Val. Loss: 0.001 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 04 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.012 | Train Acc: 59.54% | Train Recall: 5.56% | Train Precision: 9.83% | Train F2: 2.47% | Train F3: 3.24%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 05 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 67.75% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Test Loss: 0.001 | Test Acc: 98.49% | Test Recall: 0.00% | Test Precision: 0.00% | Test F2: 0.00% | Test F3: 0.00%\n",
            "\n",
            "FOLD 8 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 61.94% | Train Recall: 43.10% | Train Precision: 27.24% | Train F2: 35.78% | Train F3: 38.94%\n",
            "\t Val. Loss: 0.001 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.012 | Train Acc: 58.54% | Train Recall: 5.49% | Train Precision: 7.89% | Train F2: 4.00% | Train F3: 4.47%\n",
            "\t Val. Loss: 0.009 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.009 | Train Acc: 67.97% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 04 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.009 | Train Acc: 68.02% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 05 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.009 | Train Acc: 68.02% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Test Loss: 0.001 | Test Acc: 98.58% | Test Recall: 0.00% | Test Precision: 0.00% | Test F2: 0.00% | Test F3: 0.00%\n",
            "\n",
            "FOLD 9 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 66.04% | Train Recall: 4.51% | Train Precision: 1.83% | Train F2: 3.48% | Train F3: 3.93%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 68.02% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 68.02% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.003 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 04 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.009 | Train Acc: 72.75% | Train Recall: 20.61% | Train Precision: 24.47% | Train F2: 20.20% | Train F3: 20.36%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 98.38% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 05 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 62.91% | Train Recall: 23.40% | Train Precision: 19.46% | Train F2: 20.45% | Train F3: 21.66%\n",
            "\t Val. Loss: 0.003 |  Val. Acc: 90.75% | Val. Recall: 16.67% | Val. Precision: 3.85% | Val. F2: 10.00% | Val. F3: 12.50%\n",
            "Test Loss: 0.003 | Test Acc: 93.31% | Test Recall: 41.67% | Test Precision: 10.14% | Test F2: 23.84% | Test F3: 29.63%\n",
            "\n",
            "FOLD 10 \n",
            "\n",
            "Epoch: 01 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.010 | Train Acc: 48.67% | Train Recall: 28.31% | Train Precision: 17.58% | Train F2: 21.93% | Train F3: 23.99%\n",
            "\t Val. Loss: 0.007 |  Val. Acc: 98.40% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.009 | Train Acc: 67.63% | Train Recall: 0.00% | Train Precision: 0.00% | Train F2: 0.00% | Train F3: 0.00%\n",
            "\t Val. Loss: 0.005 |  Val. Acc: 98.40% | Val. Recall: 0.00% | Val. Precision: 0.00% | Val. F2: 0.00% | Val. F3: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.007 | Train Acc: 85.42% | Train Recall: 51.96% | Train Precision: 49.23% | Train F2: 47.70% | Train F3: 48.99%\n",
            "\t Val. Loss: 0.004 |  Val. Acc: 95.25% | Val. Recall: 25.00% | Val. Precision: 16.67% | Val. F2: 22.73% | Val. F3: 23.81%\n",
            "Epoch: 04 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.004 | Train Acc: 89.97% | Train Recall: 66.25% | Train Precision: 57.39% | Train F2: 59.48% | Train F3: 61.38%\n",
            "\t Val. Loss: 0.006 |  Val. Acc: 80.03% | Val. Recall: 100.00% | Val. Precision: 7.45% | Val. F2: 28.67% | Val. F3: 44.53%\n",
            "Epoch: 05 | Epoch Time: 0m 26s\n",
            "\tTrain Loss: 0.005 | Train Acc: 81.61% | Train Recall: 58.34% | Train Precision: 59.63% | Train F2: 52.90% | Train F3: 54.44%\n",
            "\t Val. Loss: 0.008 |  Val. Acc: 67.34% | Val. Recall: 100.00% | Val. Precision: 5.15% | Val. F2: 21.09% | Val. F3: 34.53%\n",
            "Test Loss: 0.004 | Test Acc: 96.65% | Test Recall: 21.88% | Test Precision: 22.92% | Test F2: 20.09% | Test F3: 20.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbgUS4ZLCiZG"
      },
      "source": [
        "**Notes on \"Creation of vocabulary\"**\n",
        "\n",
        "It is important to only build the vocabulary on the training set because, when testing a machine learning system, you do not want to look at the test set in any way. Also, do not include the validation set as you want it to reflect the test set as much as possible.\n",
        "\n",
        "\n",
        "In the vocabulary there is the addition of the `<pad>` token.\n",
        "\n",
        "When feeding sentences into the model, you feed a _batch_ of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Notes on \"Creation of RNN class' instance\"**\n",
        "\n",
        "- **Input dimension**: dimension of the one-hot vectors (equal to the vocabulary size).\n",
        "- **Embedding dimension**: size of the dense word vectors. This is usually around 50-250 dimensions, but depends on the size of the vocabulary.\n",
        "- **Hidden dimension**: size of the hidden states. This is usually around 100-500 dimensions, but also depends on factors such as on the vocabulary size, the size of the dense vectors and the complexity of the task.\n",
        "- **Output dimension**: usually the number of classes. However in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar real number.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Notes on \"Use of word embedding in the model\"**\n",
        "\n",
        "Replacing the initial weights of the embedding layer with the pre-trained embeddings should always be done on the `weight.data` and not the `weight`.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Notes on \"Initialization of \\<unk> and \\<pad> to all zeros\"**\n",
        "\n",
        "As our `<unk>` and `<pad>` token aren't in the pre-trained vocabulary they have been initialized using `unk_init` (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. It is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining relevance. \n",
        "\n",
        "We do this by manually setting their row in the embedding weights matrix to zeros. We get their row by finding the index of the tokens, which we have already done for the padding index.\n",
        "\n",
        "Like initializing the embeddings, this should be done on the `weight.data` and not the `weight`.\n",
        "\n",
        "As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Notes on \"Training parameters\"**\n",
        "\n",
        "**Optimizers** \n",
        "- **SGD**: updates all parameters with the same learning rate and choosing this learning rate can be tricky. \n",
        "- **Adam**: adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. \n",
        "You do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile default initial learning rate.\n",
        "\n",
        "The model currently outputs an unbound real number. As the labels are either 0 or 1, you want to restrict the predictions to a number between 0 and 1. This can be done using the _sigmoid_ or _logit_ functions. \n",
        "\n",
        "Is it possible to use the bound scalar to calculate the loss using binary cross entropy. \n",
        "\n",
        "The `BCEWithLogitsLoss` criterion carries out both the sigmoid and the binary cross entropy steps.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Notes on \"Training the model\"**\n",
        "\n",
        "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the training and validation sets.\n",
        "\n",
        "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Neural_Network_approach.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}